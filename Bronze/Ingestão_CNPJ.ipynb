{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b9a9509-b815-4357-8d6c-73f127301aba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IngestaoCNPJ\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59d278be-3c6c-4875-ad6c-1573073ddb7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Intancia_Containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d1b63b1-cf28-4ba9-9369-85e81206af27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "def unzip_file(path_origem, container_nome):\n",
    "    \"\"\"\n",
    "    Copia o ZIP do Blob Storage para o DBFS local e extrai os CSVs.\n",
    "    \"\"\"\n",
    "\n",
    "    # Nome do arquivo\n",
    "    zip_name = os.path.basename(path_origem)\n",
    "\n",
    "    # Caminho tempor√°rio no DBFS\n",
    "    dbfs_zip_path = f\"dbfs:/tmp/{zip_name}\"\n",
    "    local_zip_path = f\"/dbfs/tmp/{zip_name}\"\n",
    "\n",
    "    # Copia do Blob (wasbs) para DBFS\n",
    "    dbutils.fs.cp(path_origem, dbfs_zip_path, recurse=False)\n",
    "\n",
    "    # Pasta de extra√ß√£o\n",
    "    extract_path = f\"/dbfs/tmp/unzipped/{container_nome}\"\n",
    "    os.makedirs(extract_path, exist_ok=True)\n",
    "\n",
    "    # Extrai os arquivos\n",
    "    with zipfile.ZipFile(local_zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_path)\n",
    "\n",
    "        extracted_files = [\n",
    "            f\"/dbfs/tmp/unzipped/{container_nome}/{f}\"\n",
    "            for f in zip_ref.namelist()\n",
    "        ]\n",
    "\n",
    "    # Retorna caminhos compat√≠veis com Spark\n",
    "    return [f.replace(\"/dbfs\", \"dbfs:\") for f in extracted_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe0449f9-8fd4-4aab-9115-d9c4ceb04729",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Colunas_CNPJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5d5c028-fd06-44f6-9045-221b05ddf811",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def renomeia_colunas(df, nome_tabela):\n",
    "    # Normaliza o nome (remove n√∫meros como Empresas0, Empresas1)\n",
    "    nome_normalizado = ''.join(filter(str.isalpha, nome_tabela))\n",
    "\n",
    "    colunas_corretas = colunas_map.get(nome_normalizado)\n",
    "\n",
    "    if not colunas_corretas:\n",
    "        print(f\"[Aviso] Nenhum dicion√°rio encontrado para '{nome_tabela}'.\")\n",
    "        return df\n",
    "\n",
    "    if len(df.columns) != len(colunas_corretas):\n",
    "        print(\n",
    "            f\"[Aviso] '{nome_tabela}': n√∫mero de colunas diferente \"\n",
    "            f\"(DF={len(df.columns)} | Esperado={len(colunas_corretas)}).\"\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    # Renomeia todas as colunas de uma vez (mais perform√°tico)\n",
    "    return df.toDF(*colunas_corretas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cb6010e-391d-4f33-8f91-b7133d77532b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max as spark_max, col\n",
    "\n",
    "# Verifica ultima atualiza√ß√£o da tabela bronze\n",
    "df_last_ingestion = (\n",
    "    spark.read.format(\"delta\")\n",
    "        .load(CONTROL_TABLE_PATH)\n",
    "        .filter(col(\"originator\") == \"CNPJ\")\n",
    "        .groupBy(\"table_name\")\n",
    "        .agg(\n",
    "            spark_max(\"last_ingestion_timestamp\")\n",
    "                .alias(\"last_ingestion_timestamp\")\n",
    "        )\n",
    ")\n",
    "\n",
    "# Join com tabelas da origem\n",
    "df_inventario_filtrado = (\n",
    "    df_inventario\n",
    "        .filter(col(\"container_name\") == \"CNPJ\")\n",
    "        .join(\n",
    "            df_last_ingestion,\n",
    "            on=\"table_name\",\n",
    "            how=\"left\"\n",
    "        )\n",
    "        .filter(\n",
    "            col(\"last_ingestion_timestamp\").isNull() |\n",
    "            (col(\"modificationTime\") > col(\"last_ingestion_timestamp\"))\n",
    "        )\n",
    "        .drop(\"last_ingestion_timestamp\")\n",
    ")\n",
    "\n",
    "display(df_inventario_filtrado)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9842a65-291e-4480-98d2-1258ee695c74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# iterando nos Arquivos\n",
    "for row in df_inventario_filtrado.collect():\n",
    "    path_origem = row['path']\n",
    "    container = row['container_name']\n",
    "    nome_arquivo = row['table_name']\n",
    "\n",
    "    # Removendo a extens√£o para o nome da tabela Delta\n",
    "    tabela_nome = nome_arquivo.split('.')[0].lower()\n",
    "    print(f\"‚è≥ Processando: {tabela_nome}\")\n",
    "\n",
    "    try:\n",
    "        if not nome_arquivo.endswith('.zip'):\n",
    "            print(f\"‚ùå Arquivo {nome_arquivo} n√£o √© ZIP\")\n",
    "            continue\n",
    "\n",
    "        print(f\"üì¶ Descompactando: {nome_arquivo}\")\n",
    "        arquivos_extraidos = unzip_file(path_origem, container)\n",
    "\n",
    "        if not arquivos_extraidos:\n",
    "            print(f\"‚ö†Ô∏è Nenhum CSV encontrado em {nome_arquivo}\")\n",
    "            continue\n",
    "\n",
    "        # Remove sufixo num√©rico ‚Üí empresas0 ‚Üí empresas\n",
    "        nome_tabela = re.sub(r\"\\d+$\", \"\", tabela_nome)\n",
    "    \n",
    "        destino = f\"{BRONZE_BASE_PATH}/{container.lower()}/{nome_tabela}\"\n",
    "\n",
    "        for arquivo in arquivos_extraidos:\n",
    "            df_temp = spark.read.format(\"csv\") \\\n",
    "                .option(\"header\", \"false\") \\\n",
    "                .option(\"sep\", \";\") \\\n",
    "                .option(\"quote\", '\"') \\\n",
    "                .option(\"escape\", '\"') \\\n",
    "                .option(\"encoding\", \"ISO-8859-1\") \\\n",
    "                .load(arquivo)\n",
    "            \n",
    "            df_temp = renomeia_colunas(df_temp, nome_tabela)\n",
    "\n",
    "            df_temp = df_temp.select(\n",
    "                [col(c).cast(\"string\").alias(c) for c in df_temp.columns]\n",
    "            )\n",
    "\n",
    "            print(f\"‚úÖ Gravando tabela bronze: {nome_tabela}\")\n",
    "\n",
    "            # Adicionando metadados para qualidade dos dados\n",
    "            df = (\n",
    "                df_temp\n",
    "                .withColumn(\"_ingestion_date\", current_date())\n",
    "                .withColumn(\"_ingestion_timestamp\", current_timestamp())\n",
    "                .withColumn(\"_source_path\", lit(path_origem+\"/\"+arquivo.split(\"/\")[-1]))\n",
    "            )\n",
    "            \n",
    "            # Particionando por data de ingest√£o para manter hist√≥rico\n",
    "            df.write \\\n",
    "                .format(\"delta\") \\\n",
    "                .mode(\"append\") \\\n",
    "                .partitionBy(\"_ingestion_date\") \\\n",
    "                .save(destino)\n",
    "\n",
    "            # Atualizando tabela de controle\n",
    "            dt = DeltaTable.forPath(spark, CONTROL_TABLE_PATH)        \n",
    "            \n",
    "            controle_df = spark.createDataFrame(\n",
    "                [(\"CNPJ\", destino, path_origem)],\n",
    "                [\"originator\", \"table_name\", \"input_file_name\"]\n",
    "            ).withColumn(\n",
    "                \"last_ingestion_timestamp\", current_timestamp()\n",
    "            )\n",
    "\n",
    "            dt.alias(\"target\").merge(\n",
    "                controle_df.alias(\"source\"),\n",
    "                \"target.originator = source.originator AND target.table_name = source.table_name\"\n",
    "            ).whenMatchedUpdateAll() \\\n",
    "            .whenNotMatchedInsertAll() \\\n",
    "            .execute()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao processar {nome_arquivo}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eacbbce6-983a-4916-b339-c14d0cd05fca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "from functools import reduce\n",
    "\n",
    "tabelas_unificadas = {}\n",
    "\n",
    "for nome_tabela, lista_dfs in tabelas_bronze.items():\n",
    "    df_final = reduce(\n",
    "        lambda df1, df2: df1.unionByName(df2, allowMissingColumns=True),\n",
    "        lista_dfs\n",
    "    )\n",
    "    tabelas_unificadas[nome_tabela] = df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3db28e0-70ac-4074-83d2-f547f7f12a1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "from pyspark.sql.functions import current_timestamp, current_date, lit\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "for nome_tabela, df in tabelas_unificadas.items():\n",
    "    destino = f\"{BRONZE_BASE_PATH}/{container.lower()}/{nome_tabela}\"\n",
    "\n",
    "    print(f\"‚úÖ Gravando tabela bronze: {nome_tabela}\")\n",
    "\n",
    "    # Adicionando metadados para qualidade dos dados\n",
    "    df = (\n",
    "        df\n",
    "        .withColumn(\"_ingestion_date\", current_date())\n",
    "        .withColumn(\"_ingestion_timestamp\", current_timestamp())\n",
    "        .withColumn(\"_source_path\", lit(controle_ingestao.get(nome_tabela)))\n",
    "    )\n",
    "    \n",
    "    # Particionando por data de ingest√£o para manter hist√≥rico\n",
    "    df.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .partitionBy(\"_ingestion_date\") \\\n",
    "        .save(destino)\n",
    "\n",
    "    # Atualizando tabela de controle\n",
    "    dt = DeltaTable.forPath(spark, CONTROL_TABLE_PATH)        \n",
    "    \n",
    "    controle_df = spark.createDataFrame(\n",
    "        [(\"BALANCE\", destino, path_origem)],\n",
    "        [\"originator\", \"table_name\", \"input_file_name\"]\n",
    "    ).withColumn(\n",
    "        \"last_ingestion_timestamp\", current_timestamp()\n",
    "    )\n",
    "\n",
    "    dt.alias(\"target\").merge(\n",
    "        controle_df.alias(\"source\"),\n",
    "        \"target.originator = source.originator AND target.table_name = source.table_name\"\n",
    "    ).whenMatchedUpdateAll() \\\n",
    "    .whenNotMatchedInsertAll() \\\n",
    "    .execute()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Ingest√£o_CNPJ",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
