{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b9a9509-b815-4357-8d6c-73f127301aba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    " \n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IngestaoBronze\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59d278be-3c6c-4875-ad6c-1573073ddb7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Intancia_Containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb5cb8a8-d6bb-43f0-be8a-23bf53ba3a06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lista para armazenar dicion√°rios com os metadados\n",
    "lista_arquivos_processada = []\n",
    "\n",
    "for nome, obj in containers_registry.items():\n",
    "    # Obt√©m a lista de arquivos do container\n",
    "    arquivos = obj.get_files_list()\n",
    "    \n",
    "    for arq in arquivos:\n",
    "        lista_arquivos_processada.append({\n",
    "            \"path\": arq.path,\n",
    "            \"name\": arq.name,\n",
    "            \"size\": arq.size,\n",
    "            \"modificationTime\": arq.modificationTime,\n",
    "            \"container_name\": nome\n",
    "        })\n",
    "\n",
    "    print(f\"‚úÖ Metadados coletados de: {nome.upper()}\")\n",
    "\n",
    "# Criando o DataFrame com os arquivos\n",
    "df_inventario = spark.createDataFrame(lista_arquivos_processada)\n",
    "\n",
    "# Convertendo modificationTime para um formato mais amig√°vel\n",
    "df_inventario = df_inventario.withColumn(\n",
    "    \"modificationTime\", \n",
    "    (F.col(\"modificationTime\") / 1000).cast(\"timestamp\")\n",
    ")\n",
    "\n",
    "display(df_inventario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d1b63b1-cf28-4ba9-9369-85e81206af27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "def unzip_file(path_origem, container_nome):\n",
    "    \"\"\"\n",
    "    Copia o ZIP do Blob Storage para o DBFS local e extrai os CSVs.\n",
    "    \"\"\"\n",
    "\n",
    "    # Nome do arquivo\n",
    "    zip_name = os.path.basename(path_origem)\n",
    "\n",
    "    # Caminho tempor√°rio no DBFS\n",
    "    dbfs_zip_path = f\"dbfs:/tmp/{zip_name}\"\n",
    "    local_zip_path = f\"/dbfs/tmp/{zip_name}\"\n",
    "\n",
    "    # Copia do Blob (wasbs) para DBFS\n",
    "    dbutils.fs.cp(path_origem, dbfs_zip_path, recurse=False)\n",
    "\n",
    "    # Pasta de extra√ß√£o\n",
    "    extract_path = f\"/dbfs/tmp/unzipped/{container_nome}\"\n",
    "    os.makedirs(extract_path, exist_ok=True)\n",
    "\n",
    "    # Extrai os arquivos\n",
    "    with zipfile.ZipFile(local_zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_path)\n",
    "\n",
    "        extracted_files = [\n",
    "            f\"/dbfs/tmp/unzipped/{container_nome}/{f}\"\n",
    "            for f in zip_ref.namelist()\n",
    "            #if f.endswith(\".csv\")\n",
    "        ]\n",
    "\n",
    "    # Retorna caminhos compat√≠veis com Spark\n",
    "    return [f.replace(\"/dbfs\", \"dbfs:\") for f in extracted_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe0449f9-8fd4-4aab-9115-d9c4ceb04729",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "colunas_empresas = [\n",
    "    \"cnpj_basico\",\n",
    "    \"razao_social\",\n",
    "    \"natureza_juridica\",\n",
    "    \"qualificacao_responsavel\",\n",
    "    \"capital_social\",\n",
    "    \"porte_empresa\",\n",
    "    \"ente_federativo_responsavel\"\n",
    "]\n",
    "\n",
    "colunas_estabelecimento = [\n",
    "    \"cnpj_basico\",\n",
    "    \"cnpj_ordem\",\n",
    "    \"cnpj_dv\",\n",
    "    \"identificador_matriz_filial\",\n",
    "    \"nome_fantasia\",\n",
    "    \"situacao_cadastral\",\n",
    "    \"data_situacao_cadastral\",\n",
    "    \"motivo_situacao_cadastral\",\n",
    "    \"nome_cidade_exterior\",\n",
    "    \"pais\",\n",
    "    \"data_inicio_atividade\",\n",
    "    \"cnae_fiscal_principal\",\n",
    "    \"cnae_fiscal_secundaria\",\n",
    "    \"tipo_logradouro\",\n",
    "    \"logradouro\",\n",
    "    \"numero\",\n",
    "    \"complemento\",\n",
    "    \"bairro\",\n",
    "    \"cep\",\n",
    "    \"uf\",\n",
    "    \"municipio\",\n",
    "    \"ddd_1\",\n",
    "    \"telefone_1\",\n",
    "    \"ddd_2\",\n",
    "    \"telefone_2\",\n",
    "    \"ddd_fax\",\n",
    "    \"fax\",\n",
    "    \"email\",\n",
    "    \"situacao_especial\",\n",
    "    \"data_situacao_especial\"\n",
    "]\n",
    "\n",
    "colunas_simples = [\n",
    "    \"cnpj_basico\",\n",
    "    \"opcao_simples\",\n",
    "    \"data_opcao_simples\",\n",
    "    \"data_exclusao_simples\",\n",
    "    \"opcao_mei\",\n",
    "    \"data_opcao_mei\",\n",
    "    \"data_exclusao_mei\"\n",
    "]\n",
    "\n",
    "colunas_socios = [\n",
    "    \"cnpj_basico\",\n",
    "    \"identificador_socio\",\n",
    "    \"nome_socio_razao_social\",\n",
    "    \"cpf_cnpj_socio\",\n",
    "    \"qualificacao_socio\",\n",
    "    \"data_entrada_sociedade\",\n",
    "    \"pais\",\n",
    "    \"cpf_representante_legal\",\n",
    "    \"nome_representante_legal\",\n",
    "    \"qualificacao_representante_legal\",\n",
    "    \"faixa_etaria\"\n",
    "]\n",
    "\n",
    "colunas_paises = [\n",
    "    \"codigo_pais\",\n",
    "    \"descricao_pais\"\n",
    "]\n",
    "\n",
    "colunas_municipios = [\n",
    "    \"codigo_municipio\",\n",
    "    \"descricao_municipio\"\n",
    "]\n",
    "\n",
    "colunas_qualificacoes = [\n",
    "    \"codigo_qualificacao\",\n",
    "    \"descricao_qualificacao\"\n",
    "]\n",
    "\n",
    "colunas_natureza = [\n",
    "    \"codigo_natureza_juridica\",\n",
    "    \"descricao_natureza_juridica\"\n",
    "]\n",
    "\n",
    "colunas_cnae = [\n",
    "    \"codigo_cnae\",\n",
    "    \"descricao_cnae\"\n",
    "]\n",
    "\n",
    "colunas_motivo = [\n",
    "    \"codigo_motivo\",\n",
    "    \"descricao_motivo\"\n",
    "]\n",
    "\n",
    "# Mapear nome para lista de colunas\n",
    "colunas_map = {\n",
    "    \"Cnaes\": colunas_cnae,\n",
    "    \"Empresas\": colunas_empresas,\n",
    "    \"Estabelecimentos\": colunas_estabelecimento,\n",
    "    \"Municipios\": colunas_municipios,\n",
    "    \"Naturezas\": colunas_natureza,\n",
    "    \"Paises\": colunas_paises,\n",
    "    \"Qualificacoes\": colunas_qualificacoes,\n",
    "    \"Simples\": colunas_simples,\n",
    "    \"Socios\": colunas_socios,\n",
    "    \"Motivos\": colunas_motivo\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5d5c028-fd06-44f6-9045-221b05ddf811",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def renomeia_colunas(df, nome_tabela):\n",
    "    # Normaliza o nome (remove n√∫meros como Empresas0, Empresas1)\n",
    "    nome_normalizado = ''.join(filter(str.isalpha, nome_tabela))\n",
    "\n",
    "    colunas_corretas = colunas_map.get(nome_normalizado)\n",
    "\n",
    "    if not colunas_corretas:\n",
    "        print(f\"[Aviso] Nenhum dicion√°rio encontrado para '{nome_tabela}'.\")\n",
    "        return df\n",
    "\n",
    "    if len(df.columns) != len(colunas_corretas):\n",
    "        print(\n",
    "            f\"[Aviso] '{nome_tabela}': n√∫mero de colunas diferente \"\n",
    "            f\"(DF={len(df.columns)} | Esperado={len(colunas_corretas)}).\"\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    # Renomeia todas as colunas de uma vez (mais perform√°tico)\n",
    "    return df.toDF(*colunas_corretas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a676a016-0a7a-4915-9d7a-f82773595c7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def nome_simples_tabela(nome_arquivo):\n",
    "    base = nome_arquivo.split(\".\")[0]\n",
    "    return re.sub(r\"\\d+$\", \"\", base).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78583fdd-9e64-4597-b705-217aefb668b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def adiciona_metadados(df, path_origem):\n",
    "    return (\n",
    "        df.withColumn(\"_input_file_name\", F.lit(path_origem))\n",
    "          .withColumn(\"_ingestion_timestamp\", F.current_timestamp())\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ba1e5ff-4933-4fd2-9acc-6f583e35b634",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "TGT_STORAGE_ACCOUNT = dbutils.secrets.get(scope=\"secrets-kv\", key=\"tgt-storage-account\")\n",
    "TGT_CONTAINER = dbutils.secrets.get(scope=\"secrets-kv\", key=\"tgt-container\")\n",
    "TGT_SAS_TOKEN = dbutils.secrets.get(scope=\"secrets-kv\", key=\"tgt-sas-token\")\n",
    "\n",
    "# Configuring SAS for Lakehouse Container\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.sas.{TGT_CONTAINER}.{TGT_STORAGE_ACCOUNT}.blob.core.windows.net\",\n",
    "    TGT_SAS_TOKEN\n",
    ")\n",
    "\n",
    "LAKEHOUSE_BASE_PATH = f\"wasbs://{TGT_CONTAINER}@{TGT_STORAGE_ACCOUNT}.blob.core.windows.net/bronze_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b19a42a2-1a4e-4a0d-98d2-03efa18be90f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "df_empresas = []\n",
    "df_estabelecimentos = []\n",
    "df_socios = []\n",
    "df_outros = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9842a65-291e-4480-98d2-1258ee695c74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# iterando nos Arquivos\n",
    "for row in df_inventario.collect():\n",
    "    path_origem = row['path']\n",
    "    container = row['container_name']\n",
    "    nome_arquivo = row['name']\n",
    "    \n",
    "    # Define o caminho de destino (removendo a extens√£o para o nome da tabela Delta)\n",
    "    tabela_nome = nome_arquivo.split('.')[0]    \n",
    "    print(f\"‚è≥ Processando: {tabela_nome}\")\n",
    "\n",
    "    # Definindo destino no LakeHouse    \n",
    "    tabelas_fact_balance = (\n",
    "        tabela_nome.startswith(\"EXP_\") or\n",
    "        tabela_nome.startswith(\"IMP_\")\n",
    "    )\n",
    "\n",
    "    if tabelas_fact_balance:\n",
    "        tabela_destino = re.sub(r\"_\\d{4}\", \"\", tabela_nome).lower()\n",
    "    else:\n",
    "        tabela_destino = tabela_nome.lower()\n",
    "\n",
    "    destino_tabela = f\"{LAKEHOUSE_BASE_PATH}/{container.lower()}/{tabela_destino}\"\n",
    "\n",
    "    try:\n",
    "        if container == \"BALANCE\":\n",
    "            if nome_arquivo.endswith('.csv'):\n",
    "                df_temp = spark.read.format(\"csv\") \\\n",
    "                    .option(\"header\", \"true\") \\\n",
    "                    .option(\"sep\", \";\") \\\n",
    "                    .option(\"encoding\", \"UTF-8\") \\\n",
    "                    .load(path_origem)\n",
    "                \n",
    "                df_temp = df_temp.select(\n",
    "                    [col(c).cast(\"string\").alias(c) for c in df_temp.columns]\n",
    "                )\n",
    "                \n",
    "                # Adiciona metadados de controle (linhagem de dados)\n",
    "                df_temp = adiciona_metadados(df_temp, path_origem)\n",
    "\n",
    "                if tabelas_fact_balance:\n",
    "                    print(f\"‚úÖ Gravando FACT {tabela_nome} (append | partitionBy CO_ANO)\")\n",
    "\n",
    "                    df_temp.write \\\n",
    "                        .format(\"delta\") \\\n",
    "                        .mode(\"append\") \\\n",
    "                        .partitionBy(\"CO_ANO\") \\\n",
    "                        .option(\"mergeSchema\", \"true\") \\\n",
    "                        .save(destino_tabela)\n",
    "                \n",
    "                else:\n",
    "                    print(f\"‚ôªÔ∏è Gravando DIM {tabela_nome} (overwrite)\")\n",
    "\n",
    "                    df_temp.write \\\n",
    "                        .format(\"delta\") \\\n",
    "                        .mode(\"overwrite\") \\\n",
    "                        .option(\"overwriteSchema\", \"true\") \\\n",
    "                        .save(destino_tabela)\n",
    "            else:\n",
    "                print(f\"‚ùå Arquivo {nome_arquivo} no {container} BALANCE n√£o √© CSV\")\n",
    "            \n",
    "        elif container == \"CNPJ\":\n",
    "            if nome_arquivo.endswith('.zip'):\n",
    "                print(f\"üì¶ Descompactando: {nome_arquivo}\")\n",
    "                arquivos_extraidos = unzip_file(path_origem, container)\n",
    "\n",
    "                if not arquivos_extraidos:\n",
    "                    print(f\"‚ö†Ô∏è Nenhum CSV encontrado em {nome_arquivo}\")\n",
    "                    continue\n",
    "\n",
    "                nome_tabela = nome_simples_tabela(nome_arquivo)\n",
    "\n",
    "                for arquivo in arquivos_extraidos:\n",
    "                    df_temp = spark.read.format(\"csv\") \\\n",
    "                        .option(\"header\", \"false\") \\\n",
    "                        .option(\"sep\", \";\") \\\n",
    "                        .load(path_origem)\n",
    "                    \n",
    "                    df_temp = renomeia_colunas(df_temp, nome_tabela)\n",
    "\n",
    "                    df_temp = df_temp.select(\n",
    "                        [col(c).cast(\"string\").alias(c) for c in df_temp.columns]\n",
    "                    )\n",
    "\n",
    "                    # Adiciona metadados de controle (linhagem de dados)\n",
    "                    df_temp = adiciona_metadados(df_temp, path_origem)\n",
    "\n",
    "                    if nome_tabela == \"empresas\":\n",
    "                        df_empresas.append(df_temp)\n",
    "                    elif nome_tabela == \"estabelecimentos\":\n",
    "                        df_estabelecimentos.append(df_temp)\n",
    "                    elif nome_tabela == \"socios\":\n",
    "                        df_socios.append(df_temp)\n",
    "                    else:\n",
    "                        df_outros.setdefault(nome_tabela, []).append(df_temp)\n",
    "\n",
    "            else:\n",
    "                print(f\"‚ùå Arquivo {nome_arquivo} no {container} BALANCE n√£o √© ZIP\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao processar {nome_arquivo}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eacbbce6-983a-4916-b339-c14d0cd05fca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def union_all(dfs):\n",
    "    return reduce(lambda a, b: a.unionByName(b, allowMissingColumns=True), dfs)\n",
    "\n",
    "df_empresas_final = union_all(df_empresas) if df_empresas else None\n",
    "df_estabelecimentos_final = union_all(df_estabelecimentos) if df_estabelecimentos else None\n",
    "df_socios_final = union_all(df_socios) if df_socios else None\n",
    "\n",
    "df_outros_finais = {\n",
    "    nome: union_all(dfs)\n",
    "    for nome, dfs in df_outros.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3db28e0-70ac-4074-83d2-f547f7f12a1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fun√ß√£o simples para gravar em Delta overwrite\n",
    "def write_delta_overwrite(df, destino):\n",
    "    if df is None:\n",
    "        return\n",
    "\n",
    "    df.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .save(destino)\n",
    "\n",
    "base_destino = f\"{LAKEHOUSE_BASE_PATH}/cnpj\"\n",
    "\n",
    "write_delta_overwrite(\n",
    "    df_empresas_final,\n",
    "    f\"{base_destino}/empresas\"\n",
    ")\n",
    "\n",
    "write_delta_overwrite(\n",
    "    df_estabelecimentos_final,\n",
    "    f\"{base_destino}/estabelecimentos\"\n",
    ")\n",
    "\n",
    "write_delta_overwrite(\n",
    "    df_socios_final,\n",
    "    f\"{base_destino}/socios\"\n",
    ")\n",
    "\n",
    "for nome_tabela, df in df_outros_finais.items():\n",
    "    write_delta_overwrite(\n",
    "        df,\n",
    "        f\"{base_destino}/{nome_tabela.lower()}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Ingest√£o",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
