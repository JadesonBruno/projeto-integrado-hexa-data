{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b9a9509-b815-4357-8d6c-73f127301aba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    " \n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IngestaoBronze\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59d278be-3c6c-4875-ad6c-1573073ddb7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Intancia_Containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ba1e5ff-4933-4fd2-9acc-6f583e35b634",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Recuperando Chaves de Acesso\n",
    "TGT_STORAGE_ACCOUNT = dbutils.secrets.get(scope=\"secrets-kv\", key=\"tgt-storage-account\")\n",
    "TGT_CONTAINER = dbutils.secrets.get(scope=\"secrets-kv\", key=\"tgt-container\")\n",
    "TGT_SAS_TOKEN = dbutils.secrets.get(scope=\"secrets-kv\", key=\"tgt-sas-token\")\n",
    "\n",
    "# Configuring SAS for Lakehouse Container\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.sas.{TGT_CONTAINER}.{TGT_STORAGE_ACCOUNT}.blob.core.windows.net\",\n",
    "    TGT_SAS_TOKEN\n",
    ")\n",
    "\n",
    "# Definindo Path base do LakeHouse\n",
    "LAKEHOUSE_BASE_PATH = f\"wasbs://{TGT_CONTAINER}@{TGT_STORAGE_ACCOUNT}.blob.core.windows.net/bronze_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78c3fae9-7175-4c6e-8ff9-f75986a3c9ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max as spark_max, col\n",
    "\n",
    "ultima_atualizacao_bronze = (\n",
    "    spark.table(\"bronze_1.bronze_control_table\")\n",
    "         .filter(col(\"container_name\") == \"BALANCE\")\n",
    "         .select(spark_max(\"last_ingestion_timestamp\").alias(\"ultima_atualizacao\"))\n",
    "         .collect()[0][\"ultima_atualizacao\"]\n",
    ")\n",
    "\n",
    "print(ultima_atualizacao_bronze)\n",
    "\n",
    "df_inventario = (\n",
    "    df_inventario\n",
    "        .filter(col(\"container_name\") == \"BALANCE\")\n",
    "        if ultima_atualizacao_bronze is None\n",
    "        else df_inventario\n",
    "            .filter(col(\"container_name\") == \"BALANCE\")\n",
    "            .filter(col(\"modificationTime\") > ultima_atualizacao_bronze)\n",
    ")\n",
    "\n",
    "display(df_inventario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9842a65-291e-4480-98d2-1258ee695c74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, current_timestamp\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# iterando nos Arquivos do Container Balance\n",
    "for row in df_inventario.collect():\n",
    "    path_origem = row['path']\n",
    "    container = row['container_name']\n",
    "    nome_arquivo = row['name']\n",
    "    \n",
    "    # Define o caminho de destino (removendo a extensão para o nome da tabela Delta)\n",
    "    tabela_nome = nome_arquivo.split('.')[0]    \n",
    "    print(f\"⏳ Processando: {tabela_nome}\")\n",
    "\n",
    "    # Verificando se é uma tabela de fato ou dimensão\n",
    "    tabelas_fact_balance = (\n",
    "        tabela_nome.startswith(\"EXP_\") or\n",
    "        tabela_nome.startswith(\"IMP_\")\n",
    "    )\n",
    "\n",
    "    # Se for tabela de fato, remove o ano do nome da tabela\n",
    "    if tabelas_fact_balance:\n",
    "        tabela_destino = re.sub(r\"_\\d{4}\", \"\", tabela_nome).lower()\n",
    "    else:\n",
    "        tabela_destino = tabela_nome.lower()\n",
    "\n",
    "    # Definindo o caminho de destino no LakeHouse\n",
    "    destino_tabela = f\"{LAKEHOUSE_BASE_PATH}/{container.lower()}/{tabela_destino}\"\n",
    "\n",
    "    try:\n",
    "        if nome_arquivo.endswith('.csv'):\n",
    "            # Lendo arquivo bruto\n",
    "            df_temp = spark.read.format(\"csv\") \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .option(\"sep\", \";\") \\\n",
    "                .option(\"encoding\", \"UTF-8\") \\\n",
    "                .load(path_origem)\n",
    "            \n",
    "            # Convertendo todos os campos para string\n",
    "            df_temp = df_temp.select(\n",
    "                [col(c).cast(\"string\").alias(c) for c in df_temp.columns]\n",
    "            )\n",
    "            \n",
    "            if tabelas_fact_balance:\n",
    "                print(f\"✅ Gravando FACT {tabela_nome} (append | partitionBy CO_ANO)\")\n",
    "\n",
    "                df_temp.write \\\n",
    "                    .format(\"delta\") \\\n",
    "                    .mode(\"append\") \\\n",
    "                    .partitionBy(\"CO_ANO\") \\\n",
    "                    .option(\"mergeSchema\", \"true\") \\\n",
    "                    .save(destino_tabela)\n",
    "            \n",
    "            else:\n",
    "                print(f\"♻️ Gravando DIM {tabela_nome} (overwrite)\")\n",
    "\n",
    "                df_temp.write \\\n",
    "                    .format(\"delta\") \\\n",
    "                    .mode(\"overwrite\") \\\n",
    "                    .option(\"overwriteSchema\", \"true\") \\\n",
    "                    .save(destino_tabela)\n",
    "\n",
    "            controle_df = spark.createDataFrame(\n",
    "                [(\"BALANCE\", tabela_destino, path_origem)],\n",
    "                [\"container_name\", \"table_name\", \"input_file_name\"]\n",
    "            ).withColumn(\n",
    "                \"last_ingestion_timestamp\", current_timestamp()\n",
    "            )\n",
    "\n",
    "            controle_df.write \\\n",
    "                .format(\"delta\") \\\n",
    "                .mode(\"append\") \\\n",
    "                .saveAsTable(\"bronze_1.bronze_control_table\")\n",
    "                \n",
    "        else:\n",
    "            print(f\"❌ Arquivo {nome_arquivo} no {container} BALANCE não é CSV\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erro ao processar {nome_arquivo}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8223921454565531,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Ingestão_Balance",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
