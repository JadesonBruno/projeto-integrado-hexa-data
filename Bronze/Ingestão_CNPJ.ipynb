{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b9a9509-b815-4357-8d6c-73f127301aba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    " \n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IngestaoBronze\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59d278be-3c6c-4875-ad6c-1573073ddb7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Intancia_Containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d1b63b1-cf28-4ba9-9369-85e81206af27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "def unzip_file(path_origem, container_nome):\n",
    "    \"\"\"\n",
    "    Copia o ZIP do Blob Storage para o DBFS local e extrai os CSVs.\n",
    "    \"\"\"\n",
    "\n",
    "    # Nome do arquivo\n",
    "    zip_name = os.path.basename(path_origem)\n",
    "\n",
    "    # Caminho tempor√°rio no DBFS\n",
    "    dbfs_zip_path = f\"dbfs:/tmp/{zip_name}\"\n",
    "    local_zip_path = f\"/dbfs/tmp/{zip_name}\"\n",
    "\n",
    "    # Copia do Blob (wasbs) para DBFS\n",
    "    dbutils.fs.cp(path_origem, dbfs_zip_path, recurse=False)\n",
    "\n",
    "    # Pasta de extra√ß√£o\n",
    "    extract_path = f\"/dbfs/tmp/unzipped/{container_nome}\"\n",
    "    os.makedirs(extract_path, exist_ok=True)\n",
    "\n",
    "    # Extrai os arquivos\n",
    "    with zipfile.ZipFile(local_zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_path)\n",
    "\n",
    "        extracted_files = [\n",
    "            f\"/dbfs/tmp/unzipped/{container_nome}/{f}\"\n",
    "            for f in zip_ref.namelist()\n",
    "        ]\n",
    "\n",
    "    # Retorna caminhos compat√≠veis com Spark\n",
    "    return [f.replace(\"/dbfs\", \"dbfs:\") for f in extracted_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe0449f9-8fd4-4aab-9115-d9c4ceb04729",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Colunas_CNPJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5d5c028-fd06-44f6-9045-221b05ddf811",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def renomeia_colunas(df, nome_tabela):\n",
    "    # Normaliza o nome (remove n√∫meros como Empresas0, Empresas1)\n",
    "    nome_normalizado = ''.join(filter(str.isalpha, nome_tabela))\n",
    "\n",
    "    colunas_corretas = colunas_map.get(nome_normalizado)\n",
    "\n",
    "    if not colunas_corretas:\n",
    "        print(f\"[Aviso] Nenhum dicion√°rio encontrado para '{nome_tabela}'.\")\n",
    "        return df\n",
    "\n",
    "    if len(df.columns) != len(colunas_corretas):\n",
    "        print(\n",
    "            f\"[Aviso] '{nome_tabela}': n√∫mero de colunas diferente \"\n",
    "            f\"(DF={len(df.columns)} | Esperado={len(colunas_corretas)}).\"\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    # Renomeia todas as colunas de uma vez (mais perform√°tico)\n",
    "    return df.toDF(*colunas_corretas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ba1e5ff-4933-4fd2-9acc-6f583e35b634",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "TGT_STORAGE_ACCOUNT = dbutils.secrets.get(scope=\"secrets-kv\", key=\"tgt-storage-account\")\n",
    "TGT_CONTAINER = dbutils.secrets.get(scope=\"secrets-kv\", key=\"tgt-container\")\n",
    "TGT_SAS_TOKEN = dbutils.secrets.get(scope=\"secrets-kv\", key=\"tgt-sas-token\")\n",
    "\n",
    "# Configuring SAS for Lakehouse Container\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.sas.{TGT_CONTAINER}.{TGT_STORAGE_ACCOUNT}.blob.core.windows.net\",\n",
    "    TGT_SAS_TOKEN\n",
    ")\n",
    "\n",
    "LAKEHOUSE_BASE_PATH = f\"wasbs://{TGT_CONTAINER}@{TGT_STORAGE_ACCOUNT}.blob.core.windows.net/bronze_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe4c262b-1eee-481c-bbd5-ee6a8e66b0a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max as spark_max, col\n",
    "\n",
    "ultima_atualizacao_bronze = (\n",
    "    spark.table(\"bronze_1.bronze_control_table\")\n",
    "         .filter(col(\"container_name\") == \"CNPJ\")\n",
    "         .select(spark_max(\"last_ingestion_timestamp\").alias(\"ultima_atualizacao\"))\n",
    "         .collect()[0][\"ultima_atualizacao\"]\n",
    ")\n",
    "\n",
    "print(ultima_atualizacao_bronze)\n",
    "\n",
    "df_inventario = (\n",
    "    df_inventario\n",
    "        .filter(col(\"container_name\") == \"CNPJ\")\n",
    "        if ultima_atualizacao_bronze is None\n",
    "        else df_inventario\n",
    "            .filter(col(\"container_name\") == \"CNPJ\")\n",
    "            .filter(col(\"modificationTime\") > ultima_atualizacao_bronze)\n",
    ")\n",
    "\n",
    "display(df_inventario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9842a65-291e-4480-98d2-1258ee695c74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "tabelas_bronze = {}\n",
    "controle_ingestao = {}\n",
    "\n",
    "# iterando nos Arquivos\n",
    "for row in df_inventario.collect():\n",
    "    path_origem = row['path']\n",
    "    container = row['container_name']\n",
    "    nome_arquivo = row['name']\n",
    "    \n",
    "    # Removendo a extens√£o para o nome da tabela Delta\n",
    "    tabela_nome = nome_arquivo.split('.')[0].lower()\n",
    "    print(f\"‚è≥ Processando: {tabela_nome}\")\n",
    "\n",
    "    try:\n",
    "        if not nome_arquivo.endswith('.zip'):\n",
    "            print(f\"‚ùå Arquivo {nome_arquivo} n√£o √© ZIP\")\n",
    "            continue\n",
    "\n",
    "        print(f\"üì¶ Descompactando: {nome_arquivo}\")\n",
    "        arquivos_extraidos = unzip_file(path_origem, container)\n",
    "\n",
    "        if not arquivos_extraidos:\n",
    "            print(f\"‚ö†Ô∏è Nenhum CSV encontrado em {nome_arquivo}\")\n",
    "            continue\n",
    "\n",
    "        # Remove sufixo num√©rico ‚Üí empresas0 ‚Üí empresas\n",
    "        nome_tabela = re.sub(r\"\\d+$\", \"\", tabela_nome)\n",
    "\n",
    "        for arquivo in arquivos_extraidos:\n",
    "            df_temp = spark.read.format(\"csv\") \\\n",
    "                .option(\"header\", \"false\") \\\n",
    "                .option(\"sep\", \";\") \\\n",
    "                .option(\"encoding\", \"UTF-8\") \\\n",
    "                .load(arquivo)\n",
    "            \n",
    "            df_temp = renomeia_colunas(df_temp, nome_tabela)\n",
    "\n",
    "            df_temp = df_temp.select(\n",
    "                [col(c).cast(\"string\").alias(c) for c in df_temp.columns]\n",
    "            )\n",
    "\n",
    "            # Agrupa DataFrames por tabela l√≥gica\n",
    "            tabelas_bronze.setdefault(nome_tabela, []).append(df_temp)\n",
    "\n",
    "            # Controle de ingest√£o\n",
    "            controle_ingestao.setdefault(nome_tabela, set()).add(path_origem)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao processar {nome_arquivo}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eacbbce6-983a-4916-b339-c14d0cd05fca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "tabelas_unificadas = {}\n",
    "\n",
    "for nome_tabela, lista_dfs in tabelas_bronze.items():\n",
    "    df_final = reduce(\n",
    "        lambda df1, df2: df1.unionByName(df2, allowMissingColumns=True),\n",
    "        lista_dfs\n",
    "    )\n",
    "    tabelas_unificadas[nome_tabela] = df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3db28e0-70ac-4074-83d2-f547f7f12a1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, current_date\n",
    "\n",
    "TABELAS_SNAPSHOT = {\"empresas\", \"estabelecimentos\", \"socios\", \"simples\"}\n",
    "\n",
    "for nome_tabela, df in tabelas_unificadas.items():\n",
    "    destino = f\"{LAKEHOUSE_BASE_PATH}/{container.lower()}/{nome_tabela}\"\n",
    "\n",
    "    print(f\"‚úÖ Gravando tabela bronze: {nome_tabela}\")\n",
    "\n",
    "    df = (\n",
    "        df\n",
    "        .withColumn(\"_ingestion_date\", current_date())\n",
    "    )\n",
    "    \n",
    "    # Snapshot somente para tabelas CNPJ principais\n",
    "    if nome_tabela in TABELAS_SNAPSHOT:\n",
    "        df.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .partitionBy(\"_ingestion_date\") \\\n",
    "            .save(destino)\n",
    "\n",
    "    else:\n",
    "        # ‚úÖ Demais tabelas seguem ingest√£o padr√£o\n",
    "        df.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"overwriteSchema\", \"true\") \\\n",
    "            .save(destino)\n",
    "\n",
    "    controle_df = spark.createDataFrame(\n",
    "        [(\"CNPJ\", nome_tabela, f\"{controle_ingestao.get(nome_tabela, [])}\")],\n",
    "        [\"container_name\", \"table_name\", \"input_file_name\"]\n",
    "    ).withColumn(\n",
    "        \"last_ingestion_timestamp\", current_timestamp()\n",
    "    )\n",
    "\n",
    "    controle_df.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .saveAsTable(\"bronze_1.bronze_control_table\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 424963471118757,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Ingest√£o_CNPJ",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
