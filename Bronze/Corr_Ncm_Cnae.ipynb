{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "485ae5c5-71d1-47c8-80ed-190c55a49953",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    " \n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IngestaoCorrelacao\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "213c8c01-e0d9-473c-a83b-e9efa0323ad6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Recuperando Chaves de Acesso\n",
    "TGT_STORAGE_ACCOUNT = dbutils.secrets.get(scope=\"secrets-kv\", key=\"tgt-storage-account\")\n",
    "TGT_CONTAINER = dbutils.secrets.get(scope=\"secrets-kv\", key=\"tgt-container\")\n",
    "TGT_SAS_TOKEN = dbutils.secrets.get(scope=\"secrets-kv\", key=\"tgt-sas-token\")\n",
    "\n",
    "# Configuring SAS for Lakehouse Container\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.sas.{TGT_CONTAINER}.{TGT_STORAGE_ACCOUNT}.blob.core.windows.net\",\n",
    "    TGT_SAS_TOKEN\n",
    ")\n",
    "\n",
    "# Definindo Path base do LakeHouse\n",
    "LAKEHOUSE_BASE_PATH = f\"wasbs://{TGT_CONTAINER}@{TGT_STORAGE_ACCOUNT}.blob.core.windows.net/bronze_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e174252d-9582-4791-870f-0af253d8897d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "# Definindo o caminho de origem e destino\n",
    "working_dir = os.getcwd()\n",
    "source_path = f\"file:{working_dir}/corr_ncm_cnae.csv\"\n",
    "target_path = f\"{LAKEHOUSE_BASE_PATH}/corr_ncm_cnae\"\n",
    "\n",
    "# Lendo o CSV local\n",
    "df_bronze = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"sep\", \";\") \\\n",
    "    .load(source_path)\n",
    "\n",
    "# Escrevendo no Lakehouse em formato Delta\n",
    "df_bronze.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(target_path)\n",
    "\n",
    "controle_df = spark.createDataFrame(\n",
    "    [(\"corr_ncm_cnae\", \"corr_ncm_cnae\", \"corr_ncm_cnae.csv\")],\n",
    "    [\"container_name\", \"table_name\", \"input_file_name\"]\n",
    ").withColumn(\n",
    "    \"last_ingestion_timestamp\", current_timestamp()\n",
    ")\n",
    "\n",
    "controle_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .saveAsTable(\"bronze_1.bronze_control_table\")\n",
    "\n",
    "print(f\"Carga finalizada com sucesso em: {target_path}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Corr_Ncm_Cnae",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
