{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b9a9509-b815-4357-8d6c-73f127301aba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    " \n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IngestaoDimBalance\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59d278be-3c6c-4875-ad6c-1573073ddb7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Intancia_Containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78c3fae9-7175-4c6e-8ff9-f75986a3c9ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max as spark_max, col\n",
    "\n",
    "# Verifica ultima atualização da tabela bronze\n",
    "df_last_ingestion = (\n",
    "    spark.read.format(\"delta\")\n",
    "        .load(CONTROL_TABLE_PATH)\n",
    "        .filter(col(\"originator\") == \"BALANCE\")\n",
    "        .groupBy(\"table_name\")\n",
    "        .agg(\n",
    "            spark_max(\"last_ingestion_timestamp\")\n",
    "                .alias(\"last_ingestion_timestamp\")\n",
    "        )\n",
    ")\n",
    "\n",
    "# Join com tabelas da origem\n",
    "df_inventario_filtrado = (\n",
    "    df_inventario\n",
    "        .filter(col(\"container_name\") == \"BALANCE\")\n",
    "        # REMOVE FACT (EXP / IMP → Auto Loader)\n",
    "        .filter(\n",
    "            ~(\n",
    "                col(\"table_name\").startswith(\"EXP_\") |\n",
    "                col(\"table_name\").startswith(\"IMP_\")\n",
    "            )\n",
    "        )\n",
    "        .join(\n",
    "            df_last_ingestion,\n",
    "            on=\"table_name\",\n",
    "            how=\"left\"\n",
    "        )\n",
    "        .filter(\n",
    "            col(\"last_ingestion_timestamp\").isNull() |\n",
    "            (col(\"modificationTime\") > col(\"last_ingestion_timestamp\"))\n",
    "        )\n",
    "        .drop(\"last_ingestion_timestamp\")\n",
    ")\n",
    "\n",
    "display(df_inventario_filtrado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9842a65-291e-4480-98d2-1258ee695c74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, current_date, lit\n",
    "from delta.tables import DeltaTable\n",
    "import re\n",
    "\n",
    "# iterando nos Arquivos do Container Balance\n",
    "for row in df_inventario_filtrado.collect():\n",
    "    path_origem = row['path']\n",
    "    container = row['container_name']\n",
    "    nome_arquivo = row['table_name']\n",
    "    \n",
    "    # Define o caminho de destino (removendo a extensão para o nome da tabela Delta)\n",
    "    tabela_nome = nome_arquivo.split('.')[0]    \n",
    "    print(f\"⏳ Processando: {tabela_nome}\")\n",
    "\n",
    "    tabela_destino = tabela_nome.lower()\n",
    "    \n",
    "    # Definindo o caminho de destino no LakeHouse\n",
    "    destino_tabela = f\"{BRONZE_BASE_PATH}/balancacomercial/{tabela_destino}\"\n",
    "\n",
    "    try:\n",
    "        if nome_arquivo.endswith('.csv'):\n",
    "            # Lendo arquivo bruto\n",
    "            df_temp = spark.read.format(\"csv\") \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .option(\"sep\", \";\") \\\n",
    "                .option(\"encoding\", \"ISO-8859-1\") \\\n",
    "                .load(path_origem)\n",
    "            \n",
    "            # Convertendo todos os campos para string\n",
    "            df_temp = df_temp.select(\n",
    "                [F.col(c).cast(\"string\").alias(c) for c in df_temp.columns]\n",
    "            )\n",
    "            \n",
    "            # Adicionando metadados para qualidade dos dados\n",
    "            df_temp = (\n",
    "                df_temp\n",
    "                .withColumn(\"_ingestion_date\", F.current_date())\n",
    "                .withColumn(\"_ingestion_timestamp\", F.current_timestamp())\n",
    "                .withColumn(\"_source_path\", lit(path_origem))\n",
    "            )\n",
    "\n",
    "            print(f\"Gravando FACT {tabela_nome} (append | partitionBy Ingestion_Date)\")\n",
    "\n",
    "            df_temp.write \\\n",
    "                .format(\"delta\") \\\n",
    "                .mode(\"append\") \\\n",
    "                .partitionBy(\"_ingestion_date\") \\\n",
    "                .option(\"mergeSchema\", \"true\") \\\n",
    "                .save(destino_tabela)\n",
    "\n",
    "            # Atualizando tabela de controle\n",
    "            dt = DeltaTable.forPath(spark, CONTROL_TABLE_PATH)        \n",
    "            \n",
    "            controle_df = spark.createDataFrame(\n",
    "                [(\"BALANCE\", tabela_destino, path_origem)],\n",
    "                [\"originator\", \"table_name\", \"input_file_name\"]\n",
    "            ).withColumn(\n",
    "                \"last_ingestion_timestamp\", current_timestamp()\n",
    "            )\n",
    "\n",
    "            dt.alias(\"target\").merge(\n",
    "                controle_df.alias(\"source\"),\n",
    "                \"target.originator = source.originator AND target.table_name = source.table_name\"\n",
    "            ).whenMatchedUpdateAll() \\\n",
    "            .whenNotMatchedInsertAll() \\\n",
    "            .execute()\n",
    "\n",
    "        else:\n",
    "            print(f\"❌ Arquivo {nome_arquivo} no {container} BALANCE não é CSV\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erro ao processar {nome_arquivo}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Ingestão_Dim_Balance",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
